{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccfa5b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "import os\n",
    "from pyscikit import ImgKit\n",
    "import tensorflow as tf\n",
    "from absl import logging\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cafe5356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def set_memory_growth():\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "                logical_gpus = tf.config.experimental.list_logical_devices(\n",
    "                    'GPU')\n",
    "                logging.info(\n",
    "                    \"Detect {} Physical GPUs, {} Logical GPUs.\".format(\n",
    "                        len(gpus), len(logical_gpus)))\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            logging.info(e)\n",
    "    else:\n",
    "        logging.info(\"No GPU found!\")\n",
    "# set_memory_growth()\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "set_memory_growth()\n",
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "128a9582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tiffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25d56a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data2/input/C00001_48mp_0x8_0x007f.tif | data2/target/M_Align_00001_48mp_0x8_0x007f.tif\n",
      "data2/input/C00001_48mp_0x8_0x00ff.tif | data2/target/M_Align_00001_48mp_0x8_0x00ff.tif\n",
      "data2/input/C00001_48mp_0x8_0x01ff.tif | data2/target/M_Align_00001_48mp_0x8_0x01ff.tif\n",
      "data2/input/C00001_48mp_0x8_0x03ff.tif | data2/target/M_Align_00001_48mp_0x8_0x03ff.tif\n",
      "data2/input/C00001_48mp_0x8_0x07ff.tif | data2/target/M_Align_00001_48mp_0x8_0x07ff.tif\n",
      "data2/input/C00001_48mp_0x8_0x0fff.tif | data2/target/M_Align_00001_48mp_0x8_0x0fff.tif\n",
      "data2/input/C00001_48mp_0x8_0x1fff.tif | data2/target/M_Align_00001_48mp_0x8_0x1fff.tif\n",
      "data2/input/C00001_48mp_0x8_0x2fff.tif | data2/target/M_Align_00001_48mp_0x8_0x2fff.tif\n",
      "data2/input/C00002_48mp_0x8_0x007f.tif | data2/target/M_Align_00002_48mp_0x8_0x007f.tif\n",
      "data2/input/C00002_48mp_0x8_0x00ff.tif | data2/target/M_Align_00002_48mp_0x8_0x00ff.tif\n",
      "Number of input_img_paths samples: 518\n",
      "Number of target_img_paths samples: 407\n",
      "Number of input_img_paths samples: 406\n",
      "Number of target_img_paths samples: 406\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"data2/input/\"\n",
    "target_dir = \"data2/target/\"\n",
    "\n",
    "img_size_w = 512\n",
    "img_size_h = 512\n",
    "\n",
    "\n",
    "input_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(input_dir, fname)\n",
    "        for fname in os.listdir(input_dir)\n",
    "        if fname.endswith(\".tif\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "target_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(target_dir, fname)\n",
    "        for fname in os.listdir(target_dir)\n",
    "        if fname.endswith(\".tif\") and fname.startswith(\"M_Align\")\n",
    "    ]\n",
    ")\n",
    "# remove not matched pairs \n",
    "input_img_paths2 = []\n",
    "target_img_paths2 = []\n",
    "for filepath in input_img_paths:\n",
    "    if os.path.exists(filepath.replace(\"input/C\",\"target/M_Align_\")):\n",
    "        input_img_paths2.append(filepath)\n",
    "        target_img_paths2.append(filepath.replace(\"input/C\",\"target/M_Align_\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for input_path, target_path in zip(input_img_paths2[:10], target_img_paths2[:10]):\n",
    "    print(input_path, \"|\", target_path)\n",
    "\n",
    "\n",
    "print(\"Number of input_img_paths samples:\", len(input_img_paths))\n",
    "print(\"Number of target_img_paths samples:\", len(target_img_paths))\n",
    "\n",
    "print(\"Number of input_img_paths samples:\", len(input_img_paths2))\n",
    "print(\"Number of target_img_paths samples:\", len(target_img_paths2))\n",
    "# print(input_img_paths2[-10:])\n",
    "# print(target_img_paths2[-10:])\n",
    "input_img_paths = input_img_paths2\n",
    "target_img_paths = target_img_paths2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43598a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "import PIL\n",
    "from PIL import ImageOps\n",
    "\n",
    "# Display input image #7\n",
    "# display(Image(filename=input_img_paths[9]))\n",
    "# rgb_im = PIL.Image.open(input_img_paths[7])\n",
    "# IMG = Image.fromarray(rgb_im)\n",
    "# rgb_im\n",
    "# # Display auto-contrast version of corresponding target (per-pixel categories)\n",
    "img = PIL.ImageOps.autocontrast(load_img(target_img_paths[9]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af82fdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_im = PIL.Image.open(input_img_paths[1]).resize((256,256))\n",
    "# IMG = Image.fromarray(rgb_im)\n",
    "rgb_im\n",
    "rgb_im = PIL.Image.open(target_img_paths[1]).resize((256,256))\n",
    "# IMG = Image.fromarray(rgb_im)\n",
    "rgb_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a423b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_raw(raw):\n",
    "    # pack Bayer image to 4 channels\n",
    "    im = raw.raw_image_visible.astype(np.float32)\n",
    "    im = np.maximum(im - 512, 0) / (16383 - 512)  # subtract the black level\n",
    "\n",
    "    im = np.expand_dims(im, axis=2)\n",
    "    img_shape = im.shape\n",
    "    H = img_shape[0]\n",
    "    W = img_shape[1]\n",
    "\n",
    "    out = np.concatenate((im[0:H:2, 0:W:2, :],\n",
    "                          im[0:H:2, 1:W:2, :],\n",
    "                          im[1:H:2, 1:W:2, :],\n",
    "                          im[1:H:2, 0:W:2, :]), axis=2)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd18677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "import random\n",
    "# import tiffile as tiff\n",
    "import PIL.Image as Image\n",
    "from skimage import io\n",
    "\n",
    "class OxfordPets(keras.utils.Sequence):\n",
    "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.input_img_paths = input_img_paths\n",
    "        self.target_img_paths = target_img_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_img_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
    "        xi,yi,wi,hi = random.randint(50,400),random.randint(50,400),img_size_w,img_size_h\n",
    "        i = idx * self.batch_size\n",
    "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
    "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
    "#         print(batch_input_img_paths,batch_target_img_paths)\n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"float32\")\n",
    "        for j, path in enumerate(batch_input_img_paths):\n",
    "            img = io.imread(path)\n",
    "            img = img[xi:xi+wi,yi:yi+hi]\n",
    "#             img = (img / 127.5) - 1\n",
    "            x[j] = img\n",
    "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"float32\")\n",
    "        for j, path in enumerate(batch_target_img_paths):\n",
    "            img = io.imread(path)\n",
    "            img = img[xi:xi+wi,yi:yi+hi]\n",
    "#             img = (img / 127.5) - 1\n",
    "            y[j] = np.expand_dims(img, 2)\n",
    "\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7055a0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_model(img_size, num_classes):\n",
    "#     inputs = keras.Input(shape=img_size + (3,))\n",
    "\n",
    "#     ### [First half of the network: downsampling inputs] ###\n",
    "\n",
    "#     # Entry block\n",
    "#     x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "#     previous_block_activation = x  # Set aside residual\n",
    "\n",
    "#     # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
    "#     for filters in [64, 128, 256]:\n",
    "#         x = layers.Activation(\"relu\")(x)\n",
    "#         x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "\n",
    "#         x = layers.Activation(\"relu\")(x)\n",
    "#         x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "\n",
    "#         x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "#         # Project residual\n",
    "#         residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
    "#             previous_block_activation\n",
    "#         )\n",
    "#         x = layers.add([x, residual])  # Add back residual\n",
    "#         previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "#     ### [Second half of the network: upsampling inputs] ###\n",
    "\n",
    "#     for filters in [256, 128, 64, 32]:\n",
    "#         x = layers.Activation(\"relu\")(x)\n",
    "#         x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "\n",
    "#         x = layers.Activation(\"relu\")(x)\n",
    "#         x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "\n",
    "#         x = layers.UpSampling2D(2)(x)\n",
    "\n",
    "#         # Project residual\n",
    "#         residual = layers.UpSampling2D(2)(previous_block_activation)\n",
    "#         residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
    "#         x = layers.add([x, residual])  # Add back residual\n",
    "#         previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "#     # Add a per-pixel classification layer\n",
    "#     outputs = layers.Conv2D(num_classes, 3, padding=\"same\")(x)\n",
    "# #     outputs = tensorflow.nn.depth_to_space(x, 2)\n",
    "# #     outputs = layers.MaxPooling2D(3, strides=1, padding=\"same\")(x)\n",
    "\n",
    "#     # Define the model\n",
    "#     model = keras.Model(inputs, outputs)\n",
    "#     return model\n",
    "\n",
    "\n",
    "# # Free up RAM in case the model definition cells were run multiple times\n",
    "# keras.backend.clear_session()\n",
    "\n",
    "# img_size = (256, 256)\n",
    "# # img_size = (160, 160)\n",
    "# num_classes = 1\n",
    "# batch_size = 4\n",
    "\n",
    "# # Build model\n",
    "# model = get_model(img_size, num_classes)\n",
    "# # model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94190c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "img_size = (img_size_w, img_size_h)\n",
    "# img_size = (160, 160)\n",
    "num_classes = 1\n",
    "batch_size = 8\n",
    "\n",
    "\n",
    "# Split our img paths into a training and a validation set\n",
    "val_samples = 50\n",
    "random.Random(1).shuffle(input_img_paths)\n",
    "random.Random(1).shuffle(target_img_paths)\n",
    "train_input_img_paths = input_img_paths[:-val_samples]\n",
    "train_target_img_paths = target_img_paths[:-val_samples]\n",
    "val_input_img_paths = input_img_paths[-val_samples:]\n",
    "val_target_img_paths = target_img_paths[-val_samples:]\n",
    "\n",
    "# Instantiate data Sequences for each split\n",
    "train_gen = OxfordPets(\n",
    "    batch_size, img_size, train_input_img_paths, train_target_img_paths\n",
    ")\n",
    "val_gen = OxfordPets(batch_size, img_size, val_input_img_paths, val_target_img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f06a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it = iter(train_gen)\n",
    "# x,y = next(it)\n",
    "# # display(Image.fromarray(x[0]))\n",
    "# print(x[0].shape)\n",
    "# print(y[0].shape)\n",
    "# # print(x[0][0])\n",
    "# print(x.shape)\n",
    "# print(y.shape)\n",
    "# # print(x[0][0])\n",
    "# i=0\n",
    "# print(\"input:\")\n",
    "# img = keras.preprocessing.image.array_to_img(x[i])#PIL.ImageOps.autocontrast()\n",
    "# display(img)\n",
    "# print(\"GT:\")\n",
    "# img = keras.preprocessing.image.array_to_img(y[i])#PIL.ImageOps.autocontrast()\n",
    "# display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b89171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Dropout, BatchNormalization, LeakyReLU, concatenate\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Conv2DTranspose\n",
    "\n",
    "def get_unet(rows, cols):\n",
    "    inputs = Input((rows, cols, 1))\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
    "\n",
    "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "\n",
    "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "\n",
    "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "\n",
    "    conv10 = Conv2D(1, (1, 1), activation='linear')(conv9)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[conv10])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f909638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Image.fromarray(x[0]))\n",
    "# x[0].shape\n",
    "# print(x[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6325d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Image.fromarray(y[0]))\n",
    "# # Build model\n",
    "model = get_unet(512, 512)\n",
    "model.summary()\n",
    "\n",
    "# # display(Image.fromarray(y[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c27f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model for training.\n",
    "# We use the \"sparse\" version of categorical_crossentropy\n",
    "# because our target data is integers.\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_absolute_error\")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"model.h5\", save_best_only=True)\n",
    "]\n",
    "\n",
    "# Train the model, doing validation at the end of each epoch.\n",
    "epochs = 300\n",
    "model.fit(train_gen, epochs=epochs, validation_data=val_gen, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bee8019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for all images in the validation set\n",
    "\n",
    "val_gen = OxfordPets(batch_size, img_size, val_input_img_paths, val_target_img_paths)\n",
    "\n",
    "it = iter(val_gen)\n",
    "x,y = next(it)\n",
    "\n",
    "val_preds = model.predict(x)\n",
    "# val_preds = (val_preds +1)*127.5\n",
    "img_disp=[]\n",
    "for i in range(5):\n",
    "    print(\"input:\")\n",
    "    img_input = keras.preprocessing.image.array_to_img(x[i])#PIL.ImageOps.autocontrast()\n",
    "    # display(img_input)\n",
    "    print(\"GT:\")\n",
    "    img_GT = keras.preprocessing.image.array_to_img(y[i])#PIL.ImageOps.autocontrast()\n",
    "    # display(img_GT)\n",
    "    print(\"Predict:\")\n",
    "    img_predict = keras.preprocessing.image.array_to_img(val_preds[i])#PIL.ImageOps.autocontrast()\n",
    "    # display(img_predict)\n",
    "    img_disp.append(img_input)\n",
    "    img_disp.append(img_GT)\n",
    "    img_disp.append(img_predict)\n",
    "\n",
    "\n",
    "\n",
    "canvas = displayImages(img_disp,5,3,img_size=(512,512))\n",
    "\n",
    "\n",
    "# # Generate predictions for all images in the validation set\n",
    "\n",
    "# val_gen = OxfordPets(batch_size, img_size, val_input_img_paths, val_target_img_paths)\n",
    "# val_preds = model.predict(val_gen)\n",
    "# # val_preds = (val_preds + 2) * 256\n",
    "# # print(val_preds)\n",
    "# def display_mask(i):\n",
    "#     \"\"\"Quick utility to display a model's prediction.\"\"\"\n",
    "# #     mask = np.argmax(val_preds[i], axis=-1)\n",
    "# # #     print(mask)\n",
    "# #     mask = np.expand_dims(mask, axis=-1)\n",
    "#     img = PIL.ImageOps.autocontrast(keras.preprocessing.image.array_to_img(val_preds[i]))#PIL.ImageOps.autocontrast()\n",
    "#     print(\"******\",img.size)\n",
    "#     display(img)\n",
    "\n",
    "\n",
    "# # Display results for validation image #10\n",
    "# i = 1\n",
    "\n",
    "# # Display input image\n",
    "# # display(PIL.Image(filename=val_input_img_paths[i]))\n",
    "# # PIL.Image(filename=val_input_img_paths[i])\n",
    "# rgb_im = PIL.Image.open(val_input_img_paths[i])\n",
    "# print(val_input_img_paths[i])\n",
    "# # display(rgb_im)\n",
    "# # Display ground-truth target mask\n",
    "# img = PIL.ImageOps.autocontrast(load_img(val_target_img_paths[i]))\n",
    "# display(img)\n",
    "\n",
    "# # Display mask predicted by our model\n",
    "# display_mask(i)  # Note that the model only sees inputs at 150x150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c9a6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd13d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
